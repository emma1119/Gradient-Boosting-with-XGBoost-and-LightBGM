{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fc8d8d6",
   "metadata": {},
   "source": [
    "# Why XGBoost\n",
    "XGBoost has become the de-facto algorithm for winning competitions and more and more popular for enterprise applications simply because it is extraemely powerful. The development start with AdaBoost(weighted observations, ACR) to XGBoost and LightGBM, which takes less time to train comparing to XGBoost. \n",
    "\n",
    "## XGBoost vs LightGBM\n",
    "Both techniques are based on decision tree algorithms. The biggest difference is that LightBGM splits the tree leaf with the best fit whereas XGBoost and other boosting algorithms split the tree level. \n",
    "\n",
    "LightGBM is faster, use less memory, more accurate than other boosting (be careful of overfitting), compatible with large dataset, parallel learning supported\n",
    "\n",
    "\n",
    "### GROSS & EFB\n",
    "1) LightBGM leverage 'GROSS' -gradiant based one-side sampling, which focuz on training examples result in larger gradient\n",
    "2) it also uses 'EFB'-exclusive feature bunding, which is an automatic feature selection to bundle sparse mutually exclusive features to reduce the number of features thus improve the speed. \n",
    "\n",
    "This excercise is to explore and compare the gradiant boosting techniques with the Python packagse with real life dataset.\n",
    "\n",
    "\n",
    "Relevant blogs links:\n",
    "\n",
    "https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/\n",
    "https://www.analyticsvidhya.com/blog/2017/06/which-algorithm-takes-the-crown-light-gbm-vs-xgboost/\n",
    "https://machinelearningmastery.com/gradient-boosting-with-scikit-learn-xgboost-lightgbm-and-catboost/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8efb5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import mean, std\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from lightgbm import LGBMRegressor \n",
    "\n",
    "# conda install lightgbm\n",
    "print(lightgbm.__version__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57022d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification problem (generated sample data)\n",
    "\n",
    "# step 1: define datasets\n",
    "\n",
    "# create sample data\n",
    "X, y =make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=7)\n",
    "\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "print(X, y)\n",
    "\n",
    "#step 2: define model \n",
    "model = LGBMClassifier()\n",
    "\n",
    "#fit the model on the hwole dataset\n",
    "model.fit(X,y)\n",
    "\n",
    "#Step 3: evaluate the model \n",
    "# we will use the model using repreated stratified kfold cross-validation with three reprats and 10 folds\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats =3, random_State=1)\n",
    "\n",
    "#report the mean and standard deviation od the accuracy of the model across all repreats and folds \n",
    "\n",
    "n_scpres= cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "\n",
    "#report preformance \n",
    "print('Accuracy: %.sf (%.3f)' %(mean(n_scores), std(n_scores)))\n",
    "\n",
    "\n",
    "# step 4: fit in new data \n",
    "row = [[2.56999479, -0.13019997, 3.16075093, -4.35936352, -1.61271951, -1.39352057, -2.48924933, -1.93094078, 3.26130366, 2.05692145]]\n",
    "yhar=model.predict([row])\n",
    "\n",
    "print('Predicted Class: %d' % yhat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17f28e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression problem (generated sample data)\n",
    "\n",
    "\n",
    "# step 1: define datasets\n",
    "\n",
    "# create sample data\n",
    "X, y =make_regression(n_samples=1000, n_features=20, n_informative=15, noise=0.1, random_state=7)\n",
    "\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "print(X, y)\n",
    "\n",
    "#step 2: define model \n",
    "model = LGBMRegressor()\n",
    "\n",
    "#fit the model on the hwole dataset\n",
    "model.fit(X,y)\n",
    "\n",
    "#Step 3: evaluate the model \n",
    "# we will use the model using repreated kfold cross-validation with three reprats and 10 folds\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats =3, random_State=1)\n",
    "\n",
    "\n",
    "#report the mean absolute error (MAE) of the model across all repreats and folds \n",
    "# the scikit-learn library ,ales the <AR megative so that it is maximized instead of minimized.\n",
    "# hte ;arger negative MAE the better the model performs. Perfect model has a MAE of 0.\n",
    "\n",
    "n_scpres= cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1, error_score='raise')\n",
    "\n",
    "#report preformance \n",
    "print('Accuracy: %.sf (%.3f)' %(mean(n_scores), std(n_scores)))\n",
    "\n",
    "\n",
    "# step 4: fit in new data \n",
    "row = [[2.56999479, -0.13019997, 3.16075093, -4.35936352, -1.61271951, -1.39352057, -2.48924933, -1.93094078, 3.26130366, 2.05692145]]\n",
    "yhar=model.predict([row])\n",
    "\n",
    "print('Predicted Class: %d' % yhat[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
